\section{Optimizations}
\label{sec:optimizations}


%\todo{check}

We apply four optimizations to the base algorithm to control intermediate blow‐ups in the size of both the PN and the constructed semilinear set. 
%
An extensive empirical evaluation of these optimizations appears in \Cref {sec:evaluation}.

%\medskip
%\noindent
%\textbf{(1) Bidirectional slicing.} 
\subsection{Bidirectional slicing} 
When checking for Petri net reachability, many transitions and places might be irrelevant to the specific target set that is checked~\cite{Ra12}.  
	Before symbolic reasoning, we slice irrelevant places and transitions by iteratively combining forward passes and backward passes:  
	the forward passes over-approximate the places that are reachable from the initial marking $M_0$; and symmetrically,   
	the backward passes traverse in reverse from any place that can potentially influence a target constraint (and hence, over-approximating the places that can contribute to it).
	Then, until reaching a fixed point, we iteratively remove places and transitions that are non-forward-reachable and
	non-backward-relevant.  
	This process is formally defined in Theorem~\ref{thm:bidirectional-pruning}, and is illustrated in Fig.~\ref{fig:bidirectional_pruning}. We prove the theorem's soundness in Appendix~\ref{appendix:BidirectionalProof}.




\begin{figure}[h]
	\centering
	
	% Top row: (a), (b)
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{plots/bidirectional_pruning_step_a_updated.pdf}
		\caption{Step 0: initial Petri net, before slicing.}
		\label{fig:step:a}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{plots/bidirectional_pruning_step_b_updated.pdf}
		\caption{Step 1: first forward pass.}
		\label{fig:step:b}
	\end{subfigure}
	
	\vspace{1em}
	
	% Bottom row: (c), (d), (e)
	\begin{subfigure}[b]{0.30\textwidth}
		\centering
		\includegraphics[width=\textwidth]{plots/bidirectional_pruning_step_c_updated.pdf}
		\caption{Step 2: first backward pass.}
		\label{fig:step:c}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{0.23\textwidth}
		\centering
		\includegraphics[width=\textwidth]{plots/bidirectional_pruning_step_d_updated_2.pdf}
		\caption{Step 3: second forward pass.}
		\label{fig:step:d}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{0.23\textwidth}
		\centering
		\includegraphics[width=\textwidth]{plots/bidirectional_pruning_step_e_updated_2.pdf}
		\caption{Step 4: final Petri net.}
		\label{fig:step:e}
	\end{subfigure}
	
	\caption{A Petri net during three rounds of bidirectional slicing: two forward passes and one backward pass. Black dots represent initial token markings; green places represent places that are allowed to be reachable in our constraints (i.e., aren't fixed to zero tokens in the final marking). Dashed shapes represent places and transitions that are identified as removable in the current iteration, and will be removed after it ends.}
	\label{fig:bidirectional_pruning}
\end{figure}

%\medspace
%\guy{check with mark (M0 projected on P or P'?)}
%\noindent\fbox{%
%	\begin{minipage}{\linewidth}
\begin{theorem}[Bidirectional Slicing Soundness]
	\label{thm:bidirectional-pruning}
	Let $N = (P, T, \Pre, \Post, M_0)$ be a Petri net and $S$ a target set.  
	Let $N' = (P',T',\,\Pre|_{P'\times T'},\,\Post|_{P'\times T'},\,M_0|_{P'})$ be the sliced net.  
	Then $S$ is reachable from $N$ iff it is reachable from $N'$.
\end{theorem}
%\end{minipage}%
%}
%
%We depict this in Fig.~\ref{fig:bidirectional_pruning}, and prove Theorem~\ref{thm:bidirectional-pruning} in Appendix~\ref{appendix:BidirectionalProof}.
% (see the proof in Appendix~\ref{appendix:BidirectionalProof}).
%

% \medskip
% \noindent\textit{\textbf{Intuition.}}
% Before any heavy symbolic reasoning takes place, we apply bidirectional pruning on the underlying PN.  In the forward pass, we traverse from the initial marking to identify an over-approximation of all places and transitions that could ever fire; in the backward pass, we traverse backward from any place that can influence a target constraint, and identify over-approximations on transitions and places that cannot contribute to reaching it.  By iteratively repeating forward passes and backward passes until convergence, we remove every component of the net that cannot both originate and contribute to the reachable target set.  This dramatically shrinks the net in practice, often converting an intractably large model into one small enough for exhaustive analysis.

%\medskip
%\noindent
\subsection{Semilinear set pruning}  
A semilinear set is defined as a finite union of linear sets: $S=\bigcup_{i=1}^m L_i$, with each set of the form 
	\[L_i=\{\,\mathbf{b}_i+\sum_{\mathbf{p}\in P_i}n_p \mathbf{p} \mid n_p\in\mathbb N\}.\] 
	
	\noindent
	Each linear set may contain redundant 
	period vectors or redundant components.  
	Thus, during construction of the semilinear set, we:
	(1) remove any period vector $\mathbf{p}\in P_i$ that is also expressible as a nonnegative combination of the remaining period vectors $P_i\setminus\{\mathbf{p}\}$; 
	and (2) drop a linear set $L_i$ when it does not add expressiveness, i.e., $L_i\subseteq L_j$ (for \(i \neq j\)).  
	This pruning procedure keeps the solver calls tractable by making the underlying formulas compact.




% it is common for some inequalities or disjuncts to add no new coverage beyond what other constraints already guarantee.  The redundant‐constraint elimination pass inspects each linear inequality and each disjunct in a disjunctive normal form, testing whether it is implied by the rest.  Any constraint or disjunct found redundant is dropped, ensuring that subsequent intersection, union, and projection operations work on the smallest necessary formula.  This streamlines the logic formula and prevents unnecessary size blow‐ups during solver invocations.

% %
% We replace each period‐basis \(P_i\) by
% \[
% P_i \;:=\;\{\,p\in P_i \mid p\notin\mathsf{Span}(P_i\setminus\{p\})\}
% \],
% Dropping any ``redundant'' periods, and removing any \(L_j\subseteq L_i\) for \(i\neq j\), iterating to a fixed point so no two components subsume one another.

%\medskip
%\noindent
\subsection{Generating fewer constraints}  
When applying the Parikh image of a regular expression, to generate a semilinear set,
	most regex operations can be implemented without exponential blow-ups.
	However, the Kleene star operation is an exception. Given a semilinear set $S=\bigcup_{i=1}^m L_i$,  the Kleene closure $S^\ast$ can be expressed as a semilinear set by: 
	\[
	S^\ast=\bigcup_{I \subseteq \{1,...,m\}} 
	\Big\{\sum_{i \in I} \mathbf{b}_i + \sum_{\mathbf{p} \in \bigcup_{i \in I} (P_i\cup \{\mathbf{b}_i\})} n_p \mathbf{p}\Big\},
	\]
	yielding $2^m$ components. To mitigate this exponential blow-up:
	(i) if a linear set has a period-less component ($L_i=\{\mathbf{b}_i\}$), then we factor it out, star the rest, and then add the base vector $\mathbf{b}_i$ as a period;  
	(ii) if there is a zero base ($L_i=\{\sum_{\mathbf{p}\in P_i}n_p\mathbf{p}\}$), likewise, we star the rest and add each period vector $\mathbf{p}\in P_i$ as a period vector to the resulting set.  
	In each such case, we halve the total component count and circumvent exponential blow-ups.




% Let $\mathrm{comp}(S)=\{L_1,\dots,L_m\}$ 
% be the multiset of linear components of the semilinear set 
% \(\displaystyle S=\bigcup_{i=1}^m L_i\), where each 
% \(\;L_i=b_i+\langle P_i\rangle\) with \(b_i\in\mathbb N^d\) and 
% \(P_i\subseteq\mathbb N^d\).  Define the pruning operator
% \[
% \mathrm{new}(\mathcal C)
% \;=\;
% \bigcup\bigl\{\,L\in\mathcal C \;\bigm|\;\nexists\,L'\in\mathcal C\setminus\{L\}:\;L'\subsetneq L\bigr\},
% \]
% which removes any component strictly containing another.  
% %
% \guy{Nicolas is it clear we mean that we fix their semilinear "meaning" of the regex operations? For example, + is union etc..}
% Then, we replace the naive semilinear‐set operations by
% \[
% S\;+\;T
% \;=\;
% \mathrm{new}\bigl(\mathrm{comp}(S)\,\cup\,\mathrm{comp}(T)\bigr),
% \]
% \[
% S\;\cdot\;T
% \;=\;
% \mathrm{new}\Bigl(\{\,L_i\cdot L'_j \mid L_i\in\mathrm{comp}(S),\;L'_j\in\mathrm{comp}(T)\}\Bigr),
% \]
% where for
% \(\;L_i=b_i+\langle P_i\rangle,\;L'_j=b'_j+\langle P'_j\rangle\) we set
% $
% L_i\cdot L'_j
% =\;(b_i+b'_j)\;+\;\langle\,P_i\cup P'_j\,\rangle.
% $
% Finally, for Kleene‐star and plus on the regex side one similarly applies
% \(\mathrm{new}(\cdot)\) to the collection of ``folded” components instead of
% building all intermediate ones:
% \[
% S^*
% =\mathrm{new}\Bigl(\bigcup_{k\ge0}\bigl(\mathrm{comp}(S)\bigr)^k\Bigr),
% \qquad
% S^+
% =S\cdot S^*.
% \]

% \medskip
% \noindent\textit{\textbf{Intuition.}}
% %
% During set construction --- especially when introducing new existentially‐quantified variables or combining transition effects, we selectively avoid generating any marking that would strictly dominate an already‐seen solution.  In effect, whenever a candidate disjunct would yield a superset of an existing one, it is skipped entirely.  This ``generate‐less” heuristic stops the proliferation of large, overlapping regions in the semilinear description.  In benchmarks with large state spaces, it can reduce the number of intermediate branches by orders of magnitude.

%\medskip
%\noindent
\subsection{Strategic Kleene elimination order} 
%
After generating the serializability NFA from the network system, we use \textit{Kleene's algorithm}~\cite{Kl56} to translate the NFA into a regex.
% 
However, the size of the generated semilinear set is not only impacted by \textit{how} the operations of the
	semilinear set are implemented, but also by what \textit{specific} regular
	expression input is given. 
	Differently put, a regular language may be represented by various equivalent regular expressions, each with different translation complexity.
	%
	Specifically, as the Kleene star operation can potentially cause a large blow-up in the size of the semilinear set,
	we are sensitive to the \emph{star height} of the regular expression that is generated.
	%
	Moreover, naive Kleene elimination may introduce many nested stars operations.  
	Thus, we reduce this by strategically eliminating lower-degree states first:
	\[
	q^*=\arg\min_{q\in Q}\bigl(|\delta^A_{\mathrm{in}}(q)|+|\delta^A_{\mathrm{out}}(q)|\bigr).
	\]
	 
	
	\smallskip
	\noindent
	As we demonstrate in \Cref{sec:evaluation}, these optimizations make the representations \textit{significantly} more compact, and thus,
	expedite the search procedure --- allowing us to scale and decide serializability for programs that would otherwise be intractable.


%\medskip
%\noindent
%\textbf{(4) Strategic Kleene elimination order.}  
%The size of the generated semilinear set is not only impacted by how the
%semilinear set operations are implemented, but also by what specific regular
%expression is given as input: a single regular language may be represented by a
%number of equivalent regexes, each of different complexity.
%%
%In particular, as Kleene star can cause a large blow-up in the semilinear set size,
%we are especially sensitive to the \emph{star height} of the generated regex.
%%
%We use Kleene's algorithm to convert an NFA $\mathcal A=(Q,\Sigma,\delta,q_0,F)$ to a regex, which works by repeated state elimination, choosing one state at a time.
%Naively, when eliminating states in an arbitrary order, Kleene's algorithm may generate regexes with a much greater star height than necessary---a problem
%when converting to semilinear sets.
%%
%Therefore, we heuristically optimize the state elimination order to end up with a smaller regex. Formally, we pick the next state:
%\[
%q^* = 
%\arg\min_{q\in Q'}\bigl(|\delta_{\mathrm{in}}(q)|+|\delta_{\mathrm{out}}(q)|\bigr)
%\]
%where \(Q'\subseteq Q\) are the states remaining to be eliminated, choosing to
%eliminate states with a smaller total degree first.
%%
%This empirically keeps the resulting regexes small.

% \medskip
% \noindent\textit{\textbf{Intuition.}}
% When converting an NFA to a single regex, we pick the next state to eliminate by heuristically choosing the  state with the fewest incoming and outgoing edges.
% This optimization allows for circumventing 
% overblown expressions resulting in naive translations, especially with regard to  Kleene closures (the “\(\mathsf{*}\)” operator).  Instead, we analyze the structure of sub-expressions under the various operators --- estimating their branching factor, and reordering them so that simpler, low‐branching components are expanded first.  
%This adaptive ordering often leads to early detection of fixed points or dead‐ends, preventing the combinatorial explosion that arises when complex loops are expanded prematurely.  
