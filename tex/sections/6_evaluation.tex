\section{Evaluation}
\label{sec:evaluation}

%\begin{enumerate}
%    \item Benchmarks (describe our benchmarks)
%    \item Results (total time, split out SMPT time from our rust code time)
%    \item Analysis of optimizations (how much time they save, petri net sizes, semilinear set sizes)
%    \item Limitations (examples we cannot solve, future work that would help)
%\end{enumerate}


\noindent
\textbf{Experimental Setup.}
All experiments ran on a Lenovo Notebook ThinkPad P16s, with 16 AMD cores and a RAM of 58 GB, with a Linux Ubuntu 24.04.2 operating system.
%
%We plan on making all our code, benchmarks raw results publicly available with the final version of this paper.
%
Our code, benchmarks, and experimental results are publicly available~\cite{ArtifactRepository}.
%
%All experiments were run with a \texttt{TIMEOUT} value of $150$ seconds.
 



\subsection{Runtime Results}
We ran our tool on all 47 benchmarks, out of which 27 are serializable, and the remaining 20 are non serializable. 
For each benchmark, we measured the time for deciding the reachability query, as well as the overall time, including validation of the invariant proof (if serializable) or of the counterexample validation (if not serializable). These experiments ran in parallel, with $16$ cores, with all four optimizations and a \texttt{TIMEOUT} threshold of $500$ seconds. As can be seen by our results (summarized in Table~\ref{tab:benchmarks-all}), withing this time limit, our tool fully solved 26/27 serializable benchmarks 
and 19/20 non serializable benchmarks. 
%
The \textit{median} total runtime was $2{,}080$ ms across all benchmarks, and $2{,}238.50$ ms ($830$ ms) when focusing on solely serializable (non-serializable) benchmarks.
%
The \textit{average} total runtime was $52{,}857.55$ ms across all benchmarks, and $25{,}530.69$ ms ($42{,}980.47$ ms) when focusing on solely serializable (non-serializable) benchmarks.
%
We also note that when analyzing the benchmarks based on their serializability, there is a clear difference in their average runtime --- while in the serializable benchmarks the validation of the certificate takes significantly longer than generating the certificate (i.e., the proof) --- in the non serializable benchmarks this trend is reversed, with the overall time being dominated by the generation of the counterexample. This of course is not surprising, as counterexample generation can be done in polynomial time by emulating our network system and checking that the final counterexample can indeed be attained.
%
We report the full results in Table~\ref{tab:stats-summary} and Table~\ref{tab:benchmarks-all}.
%, and elaborate on the per-benchmark results in Table~\ref{tab:benchmarks-all}.





\begin{table}[H]
	\centering
	% Load the tabular from the external file:
	\input{tables/average_and_mean_values_of_big_table.tex}
\caption{Average and median runtime. Values are rounded to the nearest integer, to reduce clutter.}
\label{tab:stats-summary}
\end{table}





%=== Overall ===
%Certificate running time:
%Average = 39613.23
%Median  = 797.00
%
%Certificate validation time:
%Average = 13244.32
%Median  = 151.00
%
%Total running time:
%Average = 52857.55
%Median  = 2080.00
%
%=== Serializable Only ===
%Certificate running time:
%Average = 2273.38
%Median  = 1178.00
%
%Certificate validation time:
%Average = 23257.31
%Median  = 1299.50
%
%Total running time:
%Average = 25530.69
%Median  = 2238.50
%
%=== Non-Serializable Only ===
%Certificate running time:
%Average = 42075.84
%Median  = 773.00
%
%Certificate validation time:
%Average = 904.63
%Median  = 78.00
%
%Total running time:
%Average = 42980.47
%Median  = 830.00
%
%=== Percentiles (Overall) ===
%Certificate running time percentiles:
%25th percentile = 553.50
%50th percentile = 797.00
%100th percentile = 502810.00
%
%Certificate validation time percentiles:
%25th percentile = 66.50
%50th percentile = 151.00
%100th percentile = 282370.00
%
%Total running time percentiles:
%25th percentile = 615.00
%50th percentile = 2080.00
%100th percentile = 503336.00
%
%=== Percentiles (Serializable) ===
%Certificate running time percentiles:
%25th percentile = 312.00
%50th percentile = 1178.00
%100th percentile = 9858.00
%
%Certificate validation time percentiles:
%25th percentile = 115.75
%50th percentile = 1299.50
%100th percentile = 282370.00
%
%Total running time percentiles:
%25th percentile = 456.50
%50th percentile = 2238.50
%100th percentile = 292228.00
%
%=== Percentiles (Non-Serializable) ===
%Certificate running time percentiles:
%25th percentile = 628.50
%50th percentile = 773.00
%100th percentile = 356195.00
%
%Certificate validation time percentiles:
%25th percentile = 50.00
%50th percentile = 78.00
%100th percentile = 15227.00
%
%Total running time percentiles:
%25th percentile = 707.00
%50th percentile = 830.00
%100th percentile = 356299.00





\begin{table}[htbp]
	\centering
	% Load the tabular from the external file:
	\input{tables/big_table_summary.tex}
	\caption{Overview of benchmarks with combined categories and updated serializability markings.}
\label{tab:benchmarks-all}
\end{table}


\subsection{Optimization Analysis}
\label{subsec:optimization-results}

%Next of our four optimizations, and analyzed their effect on the overall runtime and space resources.
%
%All experiments were run with a \texttt{TIMEOUT} value of $150$ seconds.

\subsubsection{Runtime Optimization}

We ran all benchmarks with each of the following six optimization combinations: 
(i) without any optimization (marked [\texttt{\textbf{\text{-}\text{-}\text{-}\text{-}}}] in Fig.~\ref{fig:timeout_cumulative_solved_log}); (ii) with bidrectional pruning (marked [\texttt{\textbf{\text{B}\text{-}\text{-}\text{-}}}]); (iii) with redundant constraint elimination (marked [\texttt{\textbf{\text{-}\text{R}\text{-}\text{-}}}]); (iv) with generation of fewer constraints (marked [\texttt{\textbf{\text{-}\text{-}\text{G}\text{-}}}]);
(v) with strategic Kleene elimination(marked [\texttt{\textbf{\text{-}\text{-}\text{-}\text{S}}}]);
and finally, with (vi) all optimizations altogether (marked [\texttt{\textbf{\text{B}\text{R}\text{G}\text{S}}}]).
%
The results of the aggregated overall runtimes are presented in Fig.~\ref{fig:timeout_cumulative_solved_log}, and show that over $19\%$ more benchmarks are solved when using all optimizations, compared to running without any optimizations.
%
Not surprisingly, the best configuration is the one with all optimizations on. 
%
We also note that the best single-optimization configurations with regard to runtime are [\texttt{\textbf{\text{-}\text{-}\text{G}\text{-}}}] and [\texttt{\textbf{\text{-}\text{B}\text{-}\text{-}}}], solving over $74\%$ and $72\%$ of the benchmarks respectively. 
%
We also note that the two remaining optimizations, [\texttt{\textbf{\text{R}\text{-}\text{-}\text{-}}}] and [\texttt{\textbf{\text{-}\text{-}\text{-}\text{S}}}], performed slightly worse (although not by much) than without the optimizations when counting overall timeouts.
%
However, when comparing the combinations it seems that there an instances in which each of these optimizations \textit{strictly} improves runtime.
%
For example, there are instances such as \texttt{a3.ser} and \texttt{a7.ser}, in which the use of the redundant constraint optimization (  [\texttt{\textbf{\text{R}\text{-}\text{-}\text{-}}}])
affords a speedup of between $72.2\%-85.2\%$ over the same benchmarks without this optimization.



\begin{center}
	\begin{minipage}[htbp]{0.48\textwidth}
		\centering
		\includegraphics[width=\linewidth]{figures/cactus_plot.pdf}
		\captionof{figure}{Cumulative  solved instances (T.O.  $150$ sec).}
		\label{fig:timeout_cumulative_solved_log}
	\end{minipage}\hfill
	\begin{minipage}[htbp]{0.48\textwidth}
		\centering
		\includegraphics[width=\linewidth]{figures/petri_size_reduction_plot.pdf}
		\captionof{figure}{Petri net size reduction via  pruning.}
			%(timeout 150 seconds)
			%.}
		\label{fig:petri_size_reduction}
	\end{minipage}
\end{center}



\subsubsection{Space Optimization}

In addition to time complexity, out optimizations also allow reducing the space complexity of the two main components --- the Petri Net and the semilinear set.

\paragraph{Petri Net size reduction.}

With all optimizations enabled, we evaluated (see Fig.~\ref{fig:petri_size_reduction}) the impact of our bidirectional pruning by comparing, across all benchmarks, the average number of places and transitions before and after pruning. On average, pruning reduced the number of places \textit{by roughly half} --- dropping from $23.91$ places before pruning to $12.79$ places afterwards. It was even more effective on transitions, \textit{eliminating about two-thirds} of them: from $37.3$ transitions on average before pruning down to $12.61$ after. Note that the pre-pruning averages were computed over $47$ nets (one per benchmark), whereas the post-pruning averages span $224$ nets, since each pre-pruning net gives rise to a different pruned net per each disjunct in our reachability query. 
%These results are summarized in Fig.~\ref{fig:petri_size_reduction}.

%When running our tool with all optimizations on, we analyzed the effect of our bidirectional pruning, This was done be counting the average number of places and transitions before and after pruning, on all benchmarks.
%%
%The pruning resulted in a reduction of about half the number of original places --- from $23.91$ places on average \textit{before} pruning to $12.79$ places on average \textit{after} pruning.
%%
%The bidirectional was even more effective in reducing the transitions, as demonstrated by a removing about two thirds of the transitions --- resulting in pruned Petri Nets with $37.30$ transitions on average \textit{before} pruning to $12.61$ transitions on average \textit{after} pruning.
%%
%We note that the averaging for the pre-pruning step was done on $47$ nets, once per each benchmark, while the averaging for the post-pruning step was done on $224$ nets, as each pre-pruning net can give rise to multiple post-pruning nets, one per each disjunct in our reachability query.
%%
%These results are presented in Fig.~\ref{fig:petri_size_reduction}.


%Number of values for 'Before' bars:          47
%Number of values for 'After' bars:           224
%Average number of places before pruning:     23.91
%Average number of places after pruning:      12.79
%Average number of transitions before pruning: 37.30
%Average number of transitions after pruning:  12.61






\paragraph{Semilinear set size reduction.}

Finally, our last experiment batch analyzed the size reduction among the semilinear seta.
%
Towards this end, we ran all our benchmarks with all optimizations, to serve as a baseline. 
%
Then, we analyzed the effect of the three semilinear-related optimizations, i.e., all but the bidirectional pruning. For each of these three optimizations, we ran the benchmarks with all optimizations \textit{except} the one checked.
%
%Table~\ref{tab:semilinear-size-reduction} include a summary of the results, when comparing the semilinear set size.
%
Specifically, we compare both the average and the maximal (i) number of components; and (ii) period vectors per each component, as summarized in Table~\ref{tab:semilinear-size-reduction}.
%
As can be seen, the redundant constraint elimination and the generate-less-constraints optimizations had a highly significant effect on the number of components, with the latter being especially effective in reducing the maximal number of components to be $931$ times more compact, as well as the average number of components to be $223$ more compact, when compared to the baseline.
%
To ensure a fair comparison, we only measured benchmarks that completed under every configuration, deliberately omitting any case where the optimized semilinear set exploded beyond $2^{30}$ components and timed out. By excluding these intractable runs, our reported performance improvements actually \textit{understate} the true impact of these optimizations. This explosive growth, and the resulting timeouts, are especially pervasive among our state-machine benchmarks due to loops in their NS --- rendering even simple programs to become infeasible without optimizations.

%In fact, for both these optimizations are even more effective as, in order to conduct a fair comparison, we analyzed only benchmarks that terminated in all combinations, an hence we do not include cases in which these two optimizations rendered the original semilinear set intractable, due to having over $2^{30}$ components (!), hence timing-out and being excluded from the analysis.
%%
%This occurs pervasively in our state-machine benchmarks which have loops in their network systems --- and hence even simple programs of this category cannot be analyzed with respect to serializability.

%\todo{understand why this is related to loops in the NS}


\begin{table}[htbp]
	\centering
	% Load the tabular from the external file:
	\input{tables/semilinear_size_reduction.tex}
	\caption{Semilinear set size reduction via optimizations.}
	\label{tab:semilinear-size-reduction}
\end{table}

%\todo{Limitations?}
%Examples we cannot solve, future work that would help

%\newpage



